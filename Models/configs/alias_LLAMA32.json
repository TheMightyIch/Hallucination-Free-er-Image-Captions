{
  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "model_type": "causal",
  "GenerationType": "LLM",
  "cache_dir": "Models/cache",
  "responseLength": 100,
  "temperature": 0.7,
  "Quantization": "8bit",
  "ToDevice": "no"
}